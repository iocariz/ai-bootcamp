{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import docs\n",
    "from minsearch import Index\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = docs.read_github_data()\n",
    "pased_data = docs.parse_data(github_data)\n",
    "doc_chunks =docs.chunk_documents(pased_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 2000,\n",
       " 'content': '\\n\\n- **Export scores** as JSON or Python dictionary.\\n- **As a DataFrame**, either as a raw metrics table or by attaching scores to existing data rows.\\n- **Generate visual reports** in Jupyter, Colab, or export as HTML\\n- **Upload to Evidently Platform** to track evaluations over time\\n\\nThis exportability makes it easy to integrate Evidently into your existing workflows and pipelines ‚Äì\\xa0even if you are not using the Evidently Platform.\\n\\nHere is an example visual report showing various data quality metrics and test results. Other evaluations can be presented in the same way, or exported as raw scores:\\n\\n![](/images/concepts/report_test_preview.gif)\\n\\n**üìå Links:**\\n\\n- Quickstart for [LLM evaluation](/quickstart_llm) \\n- Quickstart for [ML evaluation](/quickstart_ml)\\n\\nOr read on through this page for conceptual introduction.\\n\\n**2. Synthetic data generation [NEW]**\\n\\n<Check>\\n  **TL;DR**: We have a nice config for structured synthetic data generation using LLMs.\\n</Check>\\n\\nPrimarily designed for LLM use cases, Evidently also helps you generate synthetic test datasets - such as RAG-style question-answer pairs from a knowledge base or synthetic inputs to cold-start your AI app testing.\\n\\n**üìå Links:** \\n- [Synthetic data](docs/library/synthetic_data_api) \\n\\n**3. Prompt optimization [NEW]**\\n\\n<Check>\\n  **TL;DR**: We help write prompts using labeled or annotated data as a target.\\n</Check>\\n\\nEvidently also includes tools for automated prompt writing. This features uses built-in evaluation capabilities to score prompt variations, optimizing them based on a target dataset and/or free-form user feedback.\\n\\nThis feature also help automatically generate LLM judge prompts to streamline the creation of custom evaluations.\\n\\n**üìå Links:** \\n- [Prompt optimization](docs/library/prompt_optimization)\\n\\n4. **Tracking and Visualization UI**\\n\\n<Check>\\n  **TL;DR**: There is also a minimal UI to store and track evaluation results.\\n</Check>\\n\\nThe Evidently library also includes a lightweight self-hostable UI for sto',\n",
       " 'title': 'Introduction',\n",
       " 'description': 'Core concepts and components of the Evidently Python library.',\n",
       " 'filename': 'docs/library/overview.mdx'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7e6fe3bdb530>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=['content', 'filename','title','description'],\n",
    ")\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search(\"data drift detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'fc0e6ce77977', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'GmgLnJSiRpGtgAw6PYkDTA', 'version': {'number': '9.1.1', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '5e94055934defa56e454868b7783b2a3b683785e', 'build_date': '2025-08-05T01:07:31.959947279Z', 'build_snapshot': False, 'lucene_version': '10.2.2', 'minimum_wire_compatibility_version': '8.19.0', 'minimum_index_compatibility_version': '8.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"start\": {\"type\": \"integer\"},\n",
    "            \"content\": {\"type\": \"text\"},\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"filename\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'evidently-docs'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"evidently-docs\"\n",
    "es_client.indices.create(index=index_name, body=index_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7223455e776e49768cc5916ec6ba9258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for doc in tqdm(doc_chunks):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query, num_results=15):\n",
    "    es_query = {\n",
    "        \"size\": num_results,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"type\": \"best_fields\",\n",
    "                \"fields\": [\"content\", \"filename\", \"title\", \"description\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=es_query)\n",
    "\n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    results = elastic_search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"data drift detection\"\n",
    "search_results = search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 7000,\n",
       "  'content': 'ch as articles.\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n\\n## Resources\\n\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\n\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n\\nAdditional links:\\n\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\n\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\n\\n* [\"My data drifted. What\\'s next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\n\\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'start': 3000,\n",
       "  'content': 'cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n</Info>\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\n\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'start': 2000,\n",
       "  'content': 'e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\n\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\n\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\n\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\n\\n<Info>\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n</Info>\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\n\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select co',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'start': 0,\n",
       "  'content': 'All Metrics and Presets that evaluate shift in data distributions use the default [Data Drift algorithm](/metrics/explainer_drift). It automatically selects the drift detection method based on the column type (text, categorical, numerical) and volume.\\n\\nYou can override the defaults by passing a custom parameter to the chosen Metric or Preset. You can modify the drift detection method (choose from 20+ available), thresholds, or both.&#x20;\\n\\nYou can also implement fully custom drift detection methods.\\n\\n**Pre-requisites**:\\n\\n* You know how to use [Data Definition ](/docs/library/data_definition)to map column types.\\n\\n* You know how to create [Reports](/docs/library/report) and run [Tests](/docs/library/tests).\\n\\n## Data drift parameters\\n\\n<Note>\\n  Setting conditions for data drift works differently from the usual Test  API (with `gt`, `lt`, etc.) This accounts for nuances like varying role of thresholds across drift detection methods, where \"greater\"  can be better or worse depending on the method.&#x20;\\n</Note>\\n\\n### Dataset-level\\n\\n**Dataset drift share**. You can set the share of drifting columns that signals **dataset drift** (default: 0.5) in the relevant Metrics or Presets. For example, to set it at 70%:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(drift_share=0.7)\\n]\\n```\\n\\nThis will detect dataset drift if over 70% columns are drifting, using defaults for each column.\\n\\n**Drift methods**. You can also specify the drift detection methods used on the column level. For example, to use PSI (Population Stability Index) for all columns in the dataset:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(drift_share=0.7, method=\"psi\")\\n]\\n```\\n\\nThis will check if over 70% columns are drifting, using PSI method with default thresholds.\\n\\n<Tip>\\n  See all available methods in the table below.\\n</Tip>\\n\\n**Drift thresholds**. You can set thresholds for each method. For example, use PSI with a threshold of 0.3 for categorical columns.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(cat_me',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'start': 4000,\n",
       "  'content': 'lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\"prediction\")` to your Report so that you see the drift in this value in a separate widget.\\n\\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\\n\\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'start': 4000,\n",
       "  'content': '                              | Defines the share of drifting columns as a condition for Dataset Drift. Default: 0.5                                                                                                                                       | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `cat_method` <br />`cat_threshold`              | Sets the drift method and/or threshold for all categorical columns.                                                                                                                                                        | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `num_method` <br />`num_threshold`              | Sets the drift method and/or threshold for all numerical columns.                                                                                                                                                          | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `per_column_method`<br />`per_column_threshold` | Sets the drift method and/or threshold for the listed columns (accepts a dictionary).                                                                                                                                      | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `text_method` <br /> `text_threshold`           | Defines the drift detection method and threshold for all text columns.                                                                                                                                                     | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n\\n## Data drift detection methods\\n\\n### Tabular data\\n\\nThe following methods apply to **tabular** data: numerical or categorical columns in data definition. Pass them using the `stattest` (or `num_stattest`, etc.) parameter.\\n\\n| StatTest                                          | Applicable to                                                  ',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'start': 13000,\n",
       "  'content': '----------------------------------------------------- |\\n| `perc_text_content_drift`<br /> Text content drift (domain classifier, with statistical hypothesis testing) | Applies only to text data. Trains a classifier model to distinguish between text in ‚Äúcurrent‚Äù and ‚Äúreference‚Äù datasets.<br /><br />**Default for text data ‚â§ 1000 objects.**      | <ul><li>returns `roc_auc` of the classifier as a `drift_score`</li><li>drift detected when `roc_auc` > possible ROC AUC of the random classifier at a set percentile</li><li>`threshold` sets the percentile of the possible ROC AUC values of the random classifier to compare against</li><li>default threshold: 0.95 (95th percentile)</li><li> `roc_auc` values can be 0 to 1 (typically 0.5 to 1); a higher value means more confident drift detection</li></ul> |\\n| `abs_text_content_drift`<br /> Text content drift (domain classifier)                                       | Applies only to text data. Trains a classifier model to distinguish between text in ‚Äúcurrent‚Äù and ‚Äúreference‚Äù datasets.<br /><br />**Default for text data when > 1000 objects.** | <ul><li>returns `roc_auc` of the classifier as a `drift_score`</li><li>drift detected when `roc_auc` > `threshold` </li><li>`threshold` sets the ROC AUC threshold</li><li>default threshold: 0.55</li><li> `roc_auc` values can be 0 to 1 (typically 0.5 to 1); a higher value means more confident drift detection</li></ul>                                                                                                                                        |\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n## Add a custom method\\n\\nIf you do not find a suitable drift detection method, you can implement a custom function:\\n\\n```',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'start': 5000,\n",
       "  'content': 'n_method`<br />`per_column_threshold` | Sets the drift method and/or threshold for the listed columns (accepts a dictionary).                                                                                                                                      | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n| `text_method` <br /> `text_threshold`           | Defines the drift detection method and threshold for all text columns.                                                                                                                                                     | `DriftedColumnsCount()`, `DataDriftPreset()`                 |\\n\\n## Data drift detection methods\\n\\n### Tabular data\\n\\nThe following methods apply to **tabular** data: numerical or categorical columns in data definition. Pass them using the `stattest` (or `num_stattest`, etc.) parameter.\\n\\n| StatTest                                          | Applicable to                                                                                                       | Drift score                                                                                          |\\n| ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\\n| `ks`<br />Kolmogorov‚ÄìSmirnov (K-S) test           | tabular data<br />only numerical <br /><br />**Default method for numerical data, if ‚â§ 1000 objects**               | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `chisquare`<br />Chi-Square test                  | tabular data<br />only categorical<br /><br />**Default method for categorical with > 2 labels, if ‚â§ 1000 objects** | returns `p_value`<br />drift detected when `p_value < threshold`<br />default threshold: 0.05        |\\n| `z`<br /> Z-tes',\n",
       "  'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'start': 0,\n",
       "  'content': \"Pending implementation for the new API.\\n\\n---\\n## Embeddings drift\\n\\n<Warning>\\n  This method is **coming soon** to the new Evidently API! Check the old docs for now.\\n</Warning>\\n\\nThe default embedding drift method is a **classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.\\n\\n* The default for **small data with \\\\<= 1000 observations** detects drift if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile.\\n\\n* The default for **larger data with > 1000 observations** detects drift if the ROC AUC > 0.55.\\n\\n**You can choose other embedding drift detection methods**. You can specify custom thresholds and parameters such as dimensionality reduction and choose from other methods, including Euclidean distance, Cosine Similarity, Maximum Mean Discrepancy, and share of drifted embeddings. You must specify this as a parameter. \\n\\n---\\n\\n**Pre-requisites**:\\n\\n* You know how to generate Reports or Test Suites with default parameters.\\n\\n* You know how to pass custom parameters for Reports or Test Suites.\\n\\n\\n# Default\\n\\nWhen you calculate embeddings drift, Evidently automatically applies the default drift detection method (‚Äúmodel‚Äù).\\n\\nIn Reports:\\n\\n```python\\nreport = Report(metrics=[\\n    EmbeddingsDriftMetric('small_subset')\\n])\\n```\\n\\nIn Test Suites:\\n\\n```python\\ntests = TestSuite(tests=[\\n    TestEmbeddingsDrift(embeddings_name='small_subset')\\n])\\n```\\n\\nIt works the same inside presets, like `DataDriftPreset`.\\n\\n# Embedding parameters - Metrics and Tests\\n\\nYou can override the defaults by passing a custom `drift_method` parameter to the relevant Metric or Test. You can define the embeddings drift detection method, the threshold, or both.\\n\\nPass the `drift_method` parameter:\\n\\n```python\\nfrom evidently.metrics.data_drift.embedding_drift_methods import model\\nreport = Report(metrics = [\\n    EmbeddingsDriftMetric('small_subset', \\n                          drift_method = model()\\n\",\n",
       "  'title': 'Customize Embedding Drift [Unpublished]',\n",
       "  'description': 'How to set embedding drift detection conditions.',\n",
       "  'noindex': 'true',\n",
       "  'filename': 'metrics/customize_embedding_drift.mdx'},\n",
       " {'start': 0,\n",
       "  'content': 'In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\\n\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\n\\n<Info>\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\n</Info>\\n\\n## How it works\\n\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\\n\\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\\n\\n* column type: categorical, numerical, text data \\n\\n* the number of observations in the reference dataset\\n\\n* the number of unique values in the column (n\\\\_unique)\\n\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\n\\n## Data requirements\\n\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\n\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\n\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\n\\n<Note>\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'start': 1000,\n",
       "  'content': ' observations in the reference dataset\\n\\n* the number of unique values in the column (n\\\\_unique)\\n\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\n\\n## Data requirements\\n\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\n\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\n\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\n\\n<Note>\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\\n</Note>\\n\\n## Dataset drift\\n\\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\\n\\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\\n\\n![](/images/metrics/preset_data_drift_2-min.png)\\n\\n## Tabular data drift\\n\\nThe following defaults apply for tabular data: numerical and categorical columns.\\n\\nFor **small data with \\\\<= 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipe',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'start': 2000,\n",
       "  'content': 'indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\\n</Note>\\n\\n## Dataset drift\\n\\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\\n\\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\\n\\n![](/images/metrics/preset_data_drift_2-min.png)\\n\\n## Tabular data drift\\n\\nThe following defaults apply for tabular data: numerical and categorical columns.\\n\\nFor **small data with \\\\<= 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n\\n* For categorical columns or numerical columns with n\\\\_unique \\\\<= 5: [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n\\n* For binary categorical features (n\\\\_unique \\\\<= 2): proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\n  All tests use a 0.95 confidence level by default. Drift score is P-value. (=\\\\< 0.05 means drift).\\n</Info>\\n\\nFor **larger data with > 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5):[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n\\n* For categorical columns or numerical with n\\\\_unique \\\\<= 5):[Jensen--Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\n  All metrics use a threshold = 0.1 by default. Drift score is distance/divergence. (>= 0.1 means drift).\\n</Info>\\n\\n**You can modify this drift detection logic**. You can select any method available in the library (PSI, K-L d',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'start': 3000,\n",
       "  'content': 'dia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n\\n* For categorical columns or numerical columns with n\\\\_unique \\\\<= 5: [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n\\n* For binary categorical features (n\\\\_unique \\\\<= 2): proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\n  All tests use a 0.95 confidence level by default. Drift score is P-value. (=\\\\< 0.05 means drift).\\n</Info>\\n\\nFor **larger data with > 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique > 5):[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n\\n* For categorical columns or numerical with n\\\\_unique \\\\<= 5):[Jensen--Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\n  All metrics use a threshold = 0.1 by default. Drift score is distance/divergence. (>= 0.1 means drift).\\n</Info>\\n\\n**You can modify this drift detection logic**. You can select any method available in the library (PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc.), specify thresholds, or pass a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).\\n\\n**Exploring drift.** You can see the distribution of each individual column inside the `DataDriftPreset` or using `ValueDrift` metric:\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nFor numerical features, you can also explore the values mapped in a plot.\\n\\n* The dark green line is the **mean**, as seen in the reference dataset.\\n\\n* The green area covers **one standard deviation** from the mean.\\n\\n![](/images/metrics/preset_data_drift_3-min.png)\\n\\nIndex is binned to 150 or uses timestamp if provided.&#x20;\\n\\n## Text data drift\\n\\nText content drift using a **domain classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.&#x20;\\n\\n![](/images/concepts/text_data_drift_domain_classifier.png)\\n\\nIf the model can confidently i',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'start': 4000,\n",
       "  'content': 'ivergence, Jensen-Shannon distance, Wasserstein distance, etc.), specify thresholds, or pass a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).\\n\\n**Exploring drift.** You can see the distribution of each individual column inside the `DataDriftPreset` or using `ValueDrift` metric:\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nFor numerical features, you can also explore the values mapped in a plot.\\n\\n* The dark green line is the **mean**, as seen in the reference dataset.\\n\\n* The green area covers **one standard deviation** from the mean.\\n\\n![](/images/metrics/preset_data_drift_3-min.png)\\n\\nIndex is binned to 150 or uses timestamp if provided.&#x20;\\n\\n## Text data drift\\n\\nText content drift using a **domain classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.&#x20;\\n\\n![](/images/concepts/text_data_drift_domain_classifier.png)\\n\\nIf the model can confidently identify which text samples belong to the ‚Äúnewer‚Äù data, you can consider that the two datasets are significantly different.\\n\\n<Info>\\n  You can read more about the domain classifier approach in the\\xa0[paper\\xa0](https://arxiv.org/pdf/1810.11953.pdf)‚ÄúFailing Loudly: An Empirical Study of Methods for Detecting Dataset Shift.‚Äù\\n</Info>\\n\\nThe drift score in this case is the ROC AUC of the resulting classifier.\\n\\nThe default for **larger data with > 1000 observations** detects drift if the ROC AUC > 0.55. The ROC AUC of the obtained classifier is directly compared against the set ROC AUC threshold. You can set a different threshold as a parameter.\\n\\nThe default for **small data with \\\\<= 1000 observations** detects drift if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile. This approach\\xa0**protects against false positive**\\xa0drift results for small datasets since we explicitly compare the classifier score against the ‚Äúbest random score‚Äù we could',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'start': 5000,\n",
       "  'content': 'dentify which text samples belong to the ‚Äúnewer‚Äù data, you can consider that the two datasets are significantly different.\\n\\n<Info>\\n  You can read more about the domain classifier approach in the\\xa0[paper\\xa0](https://arxiv.org/pdf/1810.11953.pdf)‚ÄúFailing Loudly: An Empirical Study of Methods for Detecting Dataset Shift.‚Äù\\n</Info>\\n\\nThe drift score in this case is the ROC AUC of the resulting classifier.\\n\\nThe default for **larger data with > 1000 observations** detects drift if the ROC AUC > 0.55. The ROC AUC of the obtained classifier is directly compared against the set ROC AUC threshold. You can set a different threshold as a parameter.\\n\\nThe default for **small data with \\\\<= 1000 observations** detects drift if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile. This approach\\xa0**protects against false positive**\\xa0drift results for small datasets since we explicitly compare the classifier score against the ‚Äúbest random score‚Äù we could obtain.\\xa0\\n\\n<Info>\\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\\n</Info>\\n\\nIf the drift is detected, Evidently will also calculate the\\xa0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words su',\n",
       "  'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'metrics/explainer_drift.mdx'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
